---
###############################################################################
# Evaluate host groups and gather facts
###############################################################################
- name: Load openshift_facts and update repos
  hosts: oo_masters_to_config:oo_nodes_to_config:oo_etcd_to_config:oo_lb_to_config
  roles:
  - openshift_facts
  - openshift_repos

- name: Set openshift_no_proxy_internal_hostnames
  hosts: oo_masters_to_config:oo_nodes_to_config
  tasks:
  - set_fact:
      openshift_no_proxy_internal_hostnames: "{{ hostvars | oo_select_keys(groups['oo_nodes_to_config']
                                                    | union(groups['oo_masters_to_config'])
                                                    | union(groups['oo_etcd_to_config'] | default([])))
                                                | oo_collect('openshift.common.hostname') | default([]) | join (',')
                                                }}"
    when: "{{ (openshift_http_proxy is defined or openshift_https_proxy is defined) and
            openshift_generate_no_proxy_hosts | default(True) | bool }}"

- name: Evaluate additional groups for upgrade
  hosts: localhost
  connection: local
  become: no
  tasks:
  - name: Evaluate etcd_hosts_to_backup
    add_host:
      name: "{{ item }}"
      groups: etcd_hosts_to_backup
    with_items: groups.oo_etcd_to_config if groups.oo_etcd_to_config is defined and groups.oo_etcd_to_config | length > 0 else groups.oo_first_master

###############################################################################
# Pre-upgrade checks
###############################################################################
- name: Verify upgrade can proceed
  hosts: oo_first_master
  vars:
    target_version: "{{ '1.2' if deployment_type == 'origin' else '3.1.1.900' }}"
    g_pacemaker_upgrade_url_segment: "{{ 'org/latest' if deployment_type =='origin' else '.com/enterprise/3.1' }}"
  gather_facts: no
  tasks:
  - fail:
      msg: >
        This upgrade is only supported for atomic-enterprise, origin, openshift-enterprise, and online
        deployment types
    when: deployment_type not in ['atomic-enterprise', 'origin','openshift-enterprise', 'online']

  - fail:
      msg: >
        This upgrade does not support Pacemaker:
        https://docs.openshift.{{ g_pacemaker_upgrade_url_segment }}/install_config/upgrading/pacemaker_to_native_ha.html
    when: openshift.master.cluster_method is defined and openshift.master.cluster_method == 'pacemaker'

  - fail:
      msg: >
        openshift_pkg_version is {{ openshift_pkg_version }} which is not a
        valid version for a {{ target_version }} upgrade
    when: openshift_pkg_version is defined and openshift_pkg_version.split('-',1).1 | version_compare(target_version ,'<')

  - fail:
      msg: >
        openshift_image_tag is {{ openshift_image_tag }} which is not a
        valid version for a {{ target_version }} upgrade
    when: openshift_image_tag is defined and openshift_image_tag.split('v',1).1 | version_compare(target_version ,'<')

- name: Verify master processes
  hosts: oo_masters_to_config
  roles:
  - openshift_facts
  tasks:
  - openshift_facts:
      role: master
      local_facts:
        ha: "{{ groups.oo_masters_to_config | length > 1 }}"

  - name: Ensure Master is running
    service:
      name: "{{ openshift.common.service_type }}-master"
      state: started
      enabled: yes
    when: openshift.master.ha is defined and not openshift.master.ha | bool and openshift.common.is_containerized | bool

  - name: Ensure HA Master is running
    service:
      name: "{{ openshift.common.service_type }}-master-api"
      state: started
      enabled: yes
    when: openshift.master.ha is defined and openshift.master.ha | bool and openshift.common.is_containerized | bool

  - name: Ensure HA Master is running
    service:
      name: "{{ openshift.common.service_type }}-master-controllers"
      state: started
      enabled: yes
    when: openshift.master.ha is defined and openshift.master.ha | bool and openshift.common.is_containerized | bool

- name: Verify node processes
  hosts: oo_nodes_to_config
  roles:
  - openshift_facts
  tasks:
  - name: Ensure Node is running
    service:
      name: "{{ openshift.common.service_type }}-node"
      state: started
      enabled: yes
    when: openshift.common.is_containerized | bool

- name: Verify upgrade targets
  hosts: oo_masters_to_config:oo_nodes_to_config
  vars:
    target_version: "{{ '1.2' if deployment_type == 'origin' else '3.1.1.900' }}"
    openshift_docker_hosted_registry_network: "{{ hostvars[groups.oo_first_master.0].openshift.common.portal_net }}"
    upgrading: True
  handlers:
  - include: ../../../../../roles/openshift_master/handlers/main.yml
  - include: ../../../../../roles/openshift_node/handlers/main.yml
  roles:
  # We want the cli role to evaluate so that the containerized oc/oadm wrappers
  # are modified to use the correct image tag.  However, this can trigger a
  # docker restart if new configuration is laid down which would immediately
  # pull the latest image and defeat the purpose of these tasks.
  - { role: openshift_cli }
  pre_tasks:
  - name: Clean package cache
    command: "{{ ansible_pkg_mgr }} clean all"
    when: not openshift.common.is_atomic | bool

  - set_fact:
      g_new_service_name: "{{ 'origin' if deployment_type =='origin' else 'atomic-openshift' }}"
    when: not openshift.common.is_containerized | bool

  - name: Determine available versions
    script: ../files/rpm_versions.sh {{ g_new_service_name }}
    register: g_rpm_versions_result
    when: not openshift.common.is_containerized | bool

  - set_fact:
      g_aos_versions: "{{ g_rpm_versions_result.stdout | from_yaml }}"
    when: not openshift.common.is_containerized | bool

  - name: Determine available versions
    script: ../files/openshift_container_versions.sh {{ openshift.common.service_type }}
    register: g_containerized_versions_result
    when: openshift.common.is_containerized | bool

  - set_fact:
      g_aos_versions: "{{ g_containerized_versions_result.stdout | from_yaml }}"
    when: openshift.common.is_containerized | bool

  - set_fact:
      g_new_version: "{{ g_aos_versions.curr_version.split('-', 1).0 if g_aos_versions.avail_version is none else g_aos_versions.avail_version.split('-', 1).0 }}"
    when: openshift_pkg_version is not defined

  - set_fact:
      g_new_version: "{{ openshift_pkg_version | replace('-','') }}"
    when: openshift_pkg_version is defined

  - set_fact:
      g_new_version: "{{ openshift_image_tag | replace('v','') }}"
    when: openshift_image_tag is defined

  - fail:
      msg: Verifying the correct version was found
    when: g_aos_versions.curr_version == ""

  - fail:
      msg: Verifying the correct version was found
    when: verify_upgrade_version is defined and g_new_version != verify_upgrade_version

  - include_vars: ../../../../../roles/openshift_master/vars/main.yml
    when: inventory_hostname in groups.oo_masters_to_config

  - name: Update systemd units
    include: ../../../../../roles/openshift_master/tasks/systemd_units.yml openshift_version=v{{ g_new_version }}
    when: inventory_hostname in groups.oo_masters_to_config

  - include_vars: ../../../../../roles/openshift_node/vars/main.yml
    when: inventory_hostname in groups.oo_nodes_to_config

  - name: Update systemd units
    include: ../../../../../roles/openshift_node/tasks/systemd_units.yml openshift_version=v{{ g_new_version }}
    when: inventory_hostname in groups.oo_nodes_to_config

  # Note: the version number is hardcoded here in hopes of catching potential
  # bugs in how g_aos_versions.curr_version is set
  - name: Verifying the correct version is installed for upgrade
    shell: grep 3.1.1.6 {{ item }}
    with_items:
      - /etc/sysconfig/openvswitch
      - /etc/sysconfig/{{ openshift.common.service_type }}*
    when: verify_upgrade_version is defined

  - name: Verifying the image version is used in the systemd unit
    shell: grep IMAGE_VERSION {{ item }}
    with_items:
      - /etc/systemd/system/openvswitch.service
      - /etc/systemd/system/{{ openshift.common.service_type }}*.service
    when: openshift.common.is_containerized | bool

  - fail:
      msg: This playbook requires Origin 1.1 or later
    when: deployment_type == 'origin' and g_aos_versions.curr_version | version_compare('1.1','<')

  - fail:
      msg: This playbook requires Atomic Enterprise Platform/OpenShift Enterprise 3.1 or later
    when: deployment_type == 'atomic-openshift' and g_aos_versions.curr_version | version_compare('3.1','<')

  - fail:
      msg: Upgrade packages not found
    when: openshift_image_tag is not defined and (g_aos_versions.avail_version | default(g_aos_versions.curr_version, true) | version_compare(target_version, '<'))

- name: Verify docker upgrade targets
  hosts: oo_masters_to_config:oo_nodes_to_config:oo_etcd_to_config
  tasks:
  - name: Determine available Docker
    script: ../files/rpm_versions.sh docker
    register: g_docker_version_result
    when: not openshift.common.is_atomic | bool

  - name: Determine available Docker
    shell: "rpm -q --queryformat '---\ncurr_version: %{VERSION}\navail_version: \n' docker"
    register: g_atomic_docker_version_result
    when: openshift.common.is_atomic | bool

  - set_fact:
      g_docker_version: "{{ g_docker_version_result.stdout | from_yaml }}"
    when: not openshift.common.is_atomic | bool

  - set_fact:
      g_docker_version: "{{ g_atomic_docker_version_result.stdout | from_yaml }}"
    when: openshift.common.is_atomic | bool

  - fail:
      msg: This playbook requires access to Docker 1.9 or later
    when: g_docker_version.avail_version | default(g_docker_version.curr_version, true) | version_compare('1.9','<')

  # TODO: add check to upgrade ostree to get latest Docker

  - set_fact:
      pre_upgrade_complete: True


##############################################################################
# Gate on pre-upgrade checks
##############################################################################
- name: Gate on pre-upgrade checks
  hosts: localhost
  connection: local
  become: no
  vars:
    pre_upgrade_hosts: "{{ groups.oo_masters_to_config | union(groups.oo_nodes_to_config) }}"
  tasks:
  - set_fact:
      pre_upgrade_completed: "{{ hostvars
                                 | oo_select_keys(pre_upgrade_hosts)
                                 | oo_collect('inventory_hostname', {'pre_upgrade_complete': true}) }}"
  - set_fact:
      pre_upgrade_failed: "{{ pre_upgrade_hosts | difference(pre_upgrade_completed) }}"
  - fail:
      msg: "Upgrade cannot continue. The following hosts did not complete pre-upgrade checks: {{ pre_upgrade_failed | join(',') }}"
    when: pre_upgrade_failed | length > 0

###############################################################################
# Backup etcd
###############################################################################
- name: Backup etcd
  hosts: etcd_hosts_to_backup
  vars:
    embedded_etcd: "{{ hostvars[groups.oo_first_master.0].openshift.master.embedded_etcd }}"
    timestamp: "{{ lookup('pipe', 'date +%Y%m%d%H%M%S') }}"
  roles:
  - openshift_facts
  tasks:
  # Ensure we persist the etcd role for this host in openshift_facts
  - openshift_facts:
      role: etcd
      local_facts: {}
    when: "'etcd' not in openshift"

  - stat: path=/var/lib/openshift
    register: var_lib_openshift

  - stat: path=/var/lib/origin
    register: var_lib_origin

  - name: Create origin symlink if necessary
    file: src=/var/lib/openshift/ dest=/var/lib/origin state=link
    when: var_lib_openshift.stat.exists == True and var_lib_origin.stat.exists == False

  # TODO: replace shell module with command and update later checks
  # We assume to be using the data dir for all backups.
  - name: Check available disk space for etcd backup
    shell: df --output=avail -k {{ openshift.common.data_dir }} | tail -n 1
    register: avail_disk

  # TODO: replace shell module with command and update later checks
  - name: Check current embedded etcd disk usage
    shell: du -k {{ openshift.etcd.etcd_data_dir }} | tail -n 1 | cut -f1
    register: etcd_disk_usage
    when: embedded_etcd | bool

  - name: Abort if insufficient disk space for etcd backup
    fail:
      msg: >
        {{ etcd_disk_usage.stdout }} Kb disk space required for etcd backup,
        {{ avail_disk.stdout }} Kb available.
    when: (embedded_etcd | bool) and (etcd_disk_usage.stdout|int > avail_disk.stdout|int)

  - name: Install etcd (for etcdctl)
    action: "{{ ansible_pkg_mgr }} name=etcd state=latest"
    when: not openshift.common.is_atomic | bool

  - name: Generate etcd backup
    command: >
      etcdctl backup --data-dir={{ openshift.etcd.etcd_data_dir }}
      --backup-dir={{ openshift.common.data_dir }}/etcd-backup-{{ timestamp }}

  - set_fact:
      etcd_backup_complete: True

  - name: Display location of etcd backup
    debug:
      msg: "Etcd backup created in {{ openshift.common.data_dir }}/etcd-backup-{{ timestamp }}"


##############################################################################
# Gate on etcd backup
##############################################################################
- name: Gate on etcd backup
  hosts: localhost
  connection: local
  become: no
  tasks:
  - set_fact:
      etcd_backup_completed: "{{ hostvars
                                 | oo_select_keys(groups.etcd_hosts_to_backup)
                                 | oo_collect('inventory_hostname', {'etcd_backup_complete': true}) }}"
  - set_fact:
      etcd_backup_failed: "{{ groups.etcd_hosts_to_backup | difference(etcd_backup_completed) }}"
  - fail:
      msg: "Upgrade cannot continue. The following hosts did not complete etcd backup: {{ etcd_backup_failed | join(',') }}"
    when: etcd_backup_failed | length > 0
