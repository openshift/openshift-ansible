---
- name: Upgrade playbook support for converged mode
  fail:
    msg: |
      Upgrade playbook is supported for converged mode only.
      Use manual method for independent mode.
  when: not glusterfs_is_native | bool

- include_tasks: cluster_health.yml
  vars:
    l_check_bricks: "{{ glusterfs_check_brick_size_health }}"
    glusterfs_health_timeout: 30
  when: glusterfs_is_native

- block:
  - name: List and get heketi podname
    oc_obj:
      namespace: "{{ glusterfs_namespace }}"
      kind: pod
      state: list
      selector: "glusterfs=heketi-{{ glusterfs_name }}-pod"
    register: heketi_pod_wait
    until:
    - "heketi_pod_wait.module_results.results[0]['items'] | count > 0"
    # Pod's 'Ready' status must be True
    - "heketi_pod_wait.module_results.results[0]['items'] | lib_utils_oo_collect(attribute='status.conditions') | lib_utils_oo_collect(attribute='status', filters={'type': 'Ready'}) | map('bool') | select | list | count == 1"
    delay: 10
    retries: "{{ (glusterfs_timeout | int / 10) | int }}"

  - import_tasks: heketi_set_cli.yml
    vars:
      heketi_pod: "{{ heketi_pod_wait.module_results.results[0]['items'][0] }}"

  - import_tasks: heketi_set_ro.yml
  - import_tasks: heketi_ops_pending.yml
  when: glusterfs_heketi_is_native

- import_tasks: heketi_db_backup.yml
  when: glusterfs_heketi_is_native

- import_tasks: heketi_get_key.yml
  when: glusterfs_heketi_admin_key is undefined

- name: Delete heketi resources
  oc_obj:
    namespace: "{{ glusterfs_namespace }}"
    kind: "{{ item.kind }}"
    name: "{{ item.name | default(omit) }}"
    selector: "{{ item.selector | default(omit) }}"
    state: absent
  with_items:
  - kind: "template,svc,route,dc,secret,sa"
    selector: "heketi"
  - kind: "template,svc,route,dc,secret,sa"
    selector: "deploy-heketi"
  failed_when: False

- name: Wait for heketi pod to delete
  oc_obj:
    namespace: "{{ glusterfs_namespace }}"
    kind: pod
    state: list
    selector: "heketi"
  register: heketi_pod_wait
  until:
  - "heketi_pod_wait.module_results.results[0]['items'] | count == 0"
  delay: 10
  retries: "{{ (glusterfs_timeout | int / 10) | int }}"

- set_fact:
    glusterfs_heketi_is_missing: true

- block:
  - name: Delete other glusterfs resources
    oc_obj:
      namespace: "{{ glusterfs_namespace }}"
      kind: "{{ item.kind }}"
      name: "{{ item.name | default(omit) }}"
      selector: "{{ item.selector | default(omit) }}"
      state: absent
    with_items:
    - kind: "template,svc,route,clusterrole,sa,dc"
      selector: "glusterfs"
    - kind: "clusterrolebinding"
      name: "glusterblock-provisioner"
    - kind: "clusterrolebinding"
      name: "glusterblock-{{ glusterfs_name }}-provisioner"
    failed_when: False

  # oc delete --cascade=false seems broken for DaemonSets.
  # Using curl to talk to the API directly.
  - name: Delete glusterfs daemonset w/o cascade
    shell: "curl -k -X DELETE https://localhost:8443/apis/extensions/v1beta1/namespaces/{{ glusterfs_namespace }}/daemonsets/glusterfs-{{ glusterfs_name }} -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' -H \"Accept: application/json\" -H \"Content-Type: application/json\"  --cert {{ openshift.common.config_base }}/master/admin.crt --key {{ openshift.common.config_base }}//master/admin.key"
    #shell: "{{ first_master_client_binary }} --config={{ openshift.common.config_base }}/master/admin.kubeconfig delete ds --namespace={{ glusterfs_namespace }} --cascade=false --selector=glusterfs"
    delegate_to: "{{ groups.oo_first_master.0 }}"
    failed_when: False

  - name: Get old-style GlusterFS pods
    oc_obj:
      namespace: "{{ glusterfs_namespace }}"
      kind: pod
      state: list
      selector: "glusterfs=pod"
    register: glusterfs_old_pods

  - name: Relabel old-style GlusterFS pods
    oc_label:
      name: "{{ item.metadata.name }}"
      namespace: "{{ glusterfs_namespace }}"
      kind: pod
      state: add
      labels: "[ { 'key': 'glusterfs', 'value': '{{ glusterfs_name }}'-pod } ]"
    with_items: "{{ glusterfs_old_pods.module_results.results[0]['items'] | default([]) }}"
  when: glusterfs_is_native

- import_tasks: glusterfs_common.yml

- block:
  - name: Get GlusterFS pods
    oc_obj:
      namespace: "{{ glusterfs_namespace }}"
      kind: pod
      state: list
      selector: "glusterfs={{ glusterfs_name }}-pod"
    register: glusterfs_pods_get

  - name: Set GlusterFS CLI
    set_fact:
      glusterfs_cli: "{{ glusterfs_pod_rsh }}gluster"
    vars:
      glusterfs_pod_name: "{% if glusterfs_is_native %}{{ glusterfs_pods_get.module_results.results[0]['items'][0].metadata.name }}{% endif %}"
      glusterfs_pod_rsh: "{% if glusterfs_is_native %}{{ openshift_client_binary }} --config={{ mktemp.stdout }}/admin.kubeconfig rsh --namespace={{ glusterfs_namespace }} {{ glusterfs_pod_name }} {% endif %}"

  - name: Get cluster current op version
    shell: "{{ glusterfs_cli }} volume get all cluster.op-version | grep cluster.op-version | awk '{ print $2 }'"
    register: glusterfs_op_version
    changed_when: false

  - name: Get cluster max op version
    shell: "{{ glusterfs_cli }} volume get all cluster.max-op-version | grep cluster.max-op-version | awk '{ print $2 }'"
    register: glusterfs_max_op_version
    changed_when: false

  - name: Set cluster op version on all volumes
    command: "{{ glusterfs_cli }} volume set all cluster.op-version {{ max_op_version }}"
    vars:
      op_version: "{{ glusterfs_op_version.stdout }}"
      max_op_version: "{{ glusterfs_max_op_version.stdout }}"
    when: max_op_version | int > op_version | int
  when: glusterfs_is_native
