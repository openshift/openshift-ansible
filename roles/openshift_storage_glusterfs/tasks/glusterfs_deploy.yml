---
- assert:
    that: "glusterfs_nodeselector.keys() | count == 1"
    msg: Only one GlusterFS nodeselector key pair should be provided

- assert:
    that: "glusterfs_nodes | count >= 3"
    msg: There must be at least three GlusterFS nodes specified

- name: Delete pre-existing GlusterFS resources
  oc_obj:
    namespace: "{{ glusterfs_namespace }}"
    kind: "template,daemonset"
    name: glusterfs
    state: absent
  when: glusterfs_wipe

- name: Unlabel any existing GlusterFS nodes
  oc_label:
    name: "{{ item }}"
    kind: node
    state: absent
    labels: "{{ glusterfs_nodeselector | oo_dict_to_list_of_dict }}"
  with_items: "{{ groups.all }}"
  when: glusterfs_wipe

- name: Delete pre-existing GlusterFS config
  file:
    path: /var/lib/glusterd
    state: absent
  delegate_to: "{{ item }}"
  with_items: "{{ glusterfs_nodes | default([]) }}"
  when: glusterfs_wipe

- name: Get GlusterFS storage devices state
  command: "pvdisplay -C --noheadings -o pv_name,vg_name {% for device in hostvars[item].glusterfs_devices %}{{ device }} {% endfor %}"
  register: devices_info
  delegate_to: "{{ item }}"
  with_items: "{{ glusterfs_nodes | default([]) }}"
  failed_when: False
  when: glusterfs_wipe

  # Runs "vgremove -fy <vg>; pvremove -fy <pv>" for every device found to be a physical volume.
- name: Clear GlusterFS storage device contents
  shell: "{% for line in item.stdout_lines %}{% set fields = line.split() %}{% if fields | count > 1 %}vgremove -fy {{ fields[1] }}; {% endif %}pvremove -fy {{ fields[0] }}; {% endfor %}"
  delegate_to: "{{ item.item }}"
  with_items: "{{ devices_info.results }}"
  when:
  - glusterfs_wipe
  - item.stdout_lines | count > 0

- name: Add service accounts to privileged SCC
  oc_adm_policy_user:
    user: "system:serviceaccount:{{ glusterfs_namespace }}:{{ item }}"
    resource_kind: scc
    resource_name: privileged
    state: present
  with_items:
  - 'default'
  - 'router'

- name: Label GlusterFS nodes
  oc_label:
    name: "{{ hostvars[glusterfs_host].glusterfs_hostname | default(glusterfs_host) }}"
    kind: node
    state: add
    labels: "{{ glusterfs_nodeselector | oo_dict_to_list_of_dict }}"
  with_items: "{{ glusterfs_nodes | default([]) }}"
  loop_control:
    loop_var: glusterfs_host

- name: Copy GlusterFS DaemonSet template
  copy:
    src: "{{ openshift.common.examples_content_version }}/glusterfs-template.yml"
    dest: "{{ mktemp.stdout }}/glusterfs-template.yml"

- name: Create GlusterFS template
  oc_obj:
    namespace: "{{ glusterfs_namespace }}"
    kind: template
    name: glusterfs
    state: present
    files:
    - "{{ mktemp.stdout }}/glusterfs-template.yml"

- name: Deploy GlusterFS pods
  oc_process:
    namespace: "{{ glusterfs_namespace }}"
    template_name: "glusterfs"
    create: True
    params:
      IMAGE_NAME: "{{ glusterfs_image }}"
      IMAGE_VERSION: "{{ glusterfs_version }}"

- name: Wait for GlusterFS pods
  oc_obj:
    namespace: "{{ glusterfs_namespace }}"
    kind: pod
    state: list
    selector: "glusterfs-node=pod"
  register: glusterfs_pods
  until:
  - "glusterfs_pods.results.results[0]['items'] | count > 0"
  # There must be as many pods with 'Ready' staus  True as there are nodes expecting those pods
  - "glusterfs_pods.results.results[0]['items'] | oo_collect(attribute='status.conditions') | oo_collect(attribute='status', filters={'type': 'Ready'}) | map('bool') | select | list | count == glusterfs_nodes | count"
  delay: 10
  retries: "{{ (glusterfs_timeout / 10) | int }}"
