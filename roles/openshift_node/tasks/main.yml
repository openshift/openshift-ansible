---
# TODO: allow for overriding default ports where possible
- fail:
    msg: "SELinux is disabled, This deployment type requires that SELinux is enabled."
  when: (not ansible_selinux or ansible_selinux.status != 'enabled') and deployment_type in ['enterprise', 'online', 'atomic-enterprise', 'openshift-enterprise']

- name: Set node facts
  openshift_facts:
    role: "{{ item.role }}"
    local_facts: "{{ item.local_facts }}"
  with_items:
  - role: common
    local_facts:
      # TODO: Replace this with a lookup or filter plugin.
      # TODO: Move this to the node role
      dns_ip: "{{ openshift_dns_ip
                  | default(openshift_master_cluster_vip
                  | default(None if openshift.common.version_gte_3_1_or_1_1 | bool else openshift_node_first_master_ip | default(None, true), true), true) }}"
  - role: node
    local_facts:
      annotations: "{{ openshift_node_annotations | default(none) }}"
      debug_level: "{{ openshift_node_debug_level | default(openshift.common.debug_level) }}"
      iptables_sync_period: "{{ openshift_node_iptables_sync_period | default(None) }}"
      kubelet_args: "{{ openshift_node_kubelet_args | default(None) }}"
      labels: "{{ lookup('oo_option', 'openshift_node_labels') | default( openshift_node_labels | default(none), true) }}"
      registry_url: "{{ oreg_url | default(none) }}"
      schedulable: "{{ openshift_schedulable | default(openshift_scheduleable) | default(None) }}"
      sdn_mtu: "{{ openshift_node_sdn_mtu | default(None) }}"
      storage_plugin_deps: "{{ osn_storage_plugin_deps | default(None) }}"
      set_node_ip: "{{ openshift_set_node_ip | default(None) }}"
      node_image: "{{ osn_image | default(None) }}"
      ovs_image: "{{ osn_ovs_image | default(None) }}"
      proxy_mode: "{{ openshift_node_proxy_mode | default('iptables') }}"
      local_quota_per_fsgroup: "{{ openshift_node_local_quota_per_fsgroup | default(None) }}"

# We have to add tuned-profiles in the same transaction otherwise we run into depsolving
# problems because the rpms don't pin the version properly. This was fixed in 3.1 packaging.
- name: Install Node package
  action: "{{ ansible_pkg_mgr }} name={{ openshift.common.service_type }}-node{{ openshift_version  }},tuned-profiles-{{ openshift.common.service_type }}-node{{ openshift_version  }} state=present"
  when: not openshift.common.is_containerized | bool

- name: Install sdn-ovs package
  action: "{{ ansible_pkg_mgr }} name={{ openshift.common.service_type }}-sdn-ovs{{ openshift_version }} state=present"
  when: openshift.common.use_openshift_sdn and not openshift.common.is_containerized | bool

- name: Pull node image
  command: >
    docker pull {{ openshift.node.node_image }}:{{ openshift_version }}
  when: openshift.common.is_containerized | bool

- name: Pull OpenVSwitch image
  command: >
    docker pull {{ openshift.node.ovs_image }}:{{ openshift_version }}
  when: openshift.common.is_containerized | bool and openshift.common.use_openshift_sdn | bool

- name: Install Node docker service file
  template:
    dest: "/etc/systemd/system/{{ openshift.common.service_type }}-node.service"
    src: openshift.docker.node.service
  register: install_node_result
  when: openshift.common.is_containerized | bool

- name: Create the openvswitch service env file
  template:
    src: openvswitch.sysconfig.j2
    dest: /etc/sysconfig/openvswitch
  when: openshift.common.is_containerized | bool
  register: install_ovs_sysconfig

- name: Install OpenvSwitch docker service file
  template:
    dest: "/etc/systemd/system/openvswitch.service"
    src: openvswitch.docker.service
  when: openshift.common.is_containerized | bool and openshift.common.use_openshift_sdn | bool
  notify:
  - restart openvswitch

- name: Reload systemd units
  command: systemctl daemon-reload
  when: openshift.common.is_containerized | bool and ( ( install_node_result  | changed )
    or ( install_ovs_sysconfig | changed ) )

- name: Start and enable openvswitch docker service
  service: name=openvswitch.service enabled=yes state=started
  when: openshift.common.is_containerized | bool and openshift.common.use_openshift_sdn | bool
  register: ovs_start_result

- set_fact:
    ovs_service_status_changed: "{{ ovs_start_result | changed }}"

# TODO: add the validate parameter when there is a validation command to run
- name: Create the Node config
  template:
    dest: "{{ openshift_node_config_file }}"
    src: node.yaml.v1.j2
    backup: true
    owner: root
    group: root
    mode: 0600
  notify:
  - restart node

- name: Configure Node settings
  lineinfile:
    dest: /etc/sysconfig/{{ openshift.common.service_type }}-node
    regexp: "{{ item.regex }}"
    line: "{{ item.line }}"
    create: true
  with_items:
    - regex: '^OPTIONS='
      line: "OPTIONS=--loglevel={{ openshift.node.debug_level }}"
    - regex: '^CONFIG_FILE='
      line: "CONFIG_FILE={{ openshift_node_config_file }}"
    - regex: '^IMAGE_VERSION='
      line: "IMAGE_VERSION={{ openshift_version }}"
  notify:
  - restart node

- name: Additional storage plugin configuration
  include: storage_plugins/main.yml

# Necessary because when you're on a node that's also a master the master will be
# restarted after the node restarts docker and it will take up to 60 seconds for
# systemd to start the master again
- name: Wait for master API to become available before proceeding
  # Using curl here since the uri module requires python-httplib2 and
  # wait_for port doesn't provide health information.
  command: >
    curl --silent --cacert {{ openshift.common.config_base }}/node/ca.crt
    {{ openshift_node_master_api_url }}/healthz/ready
  register: api_available_output
  until: api_available_output.stdout == 'ok'
  retries: 120
  delay: 1
  changed_when: false
  when: openshift.common.is_containerized | bool

- name: Start and enable node
  service: name={{ openshift.common.service_type }}-node enabled=yes state=started
  register: node_start_result

- set_fact:
    node_service_status_changed: "{{ node_start_result | changed }}"
